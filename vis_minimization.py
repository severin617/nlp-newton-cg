# -*- coding: utf-8 -*-
"""vis_minimization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ozW_RknLFu5rnuK37bACAhZC--LZV0Ud

# Setup
"""

# Commented out IPython magic to ensure Python compatibility.
# Tensorflow
# %tensorflow_version 1.x
import tensorflow as tf
tf.version.VERSION

import math
#from IPython.display import clear_output

import numpy as np
# import torch

import matplotlib.pyplot as plt
# import torch_optimizer as optim
# from hyperopt import fmin, hp, tpe

plt.style.use('seaborn-white')

import tikzplotlib

#clear_output()


# !pip install -i https://test.pypi.org/simple/ newton-cg==0.0.3
import newton_cg as es
#clear_output()

"""# Rosenbrock"""

def rosenbrock(vars):
    # https://en.wikipedia.org/wiki/Test_functions_for_optimization
    # x, y = tensor
    # return (1 - x) ** 2 + 100 * (y - x ** 2) ** 2
    return (1 - vars[0]) ** 2 + 100 * (vars[1] - vars[0] ** 2) ** 2

"""# Rastrigin"""

def rastrigin(vars):
    # https://en.wikipedia.org/wiki/Test_functions_for_optimization
    A = 10
    f = (
        A * 2
        + (vars[0] ** 2 - A * tf.math.cos(vars[0] * math.pi * 2))
        + (vars[1] ** 2 - A * tf.math.cos(vars[1] * math.pi * 2))
    )
    return f
def square(vars):
    # https://en.wikipedia.org/wiki/Test_functions_for_optimization
    # x, y = tensor
    # return (1 - x) ** 2 + 100 * (y - x ** 2) ** 2
    return (vars[0]) ** 2 + 4 * (vars[1]) ** 2

"""# Execute_steps"""

# a list containing a tuple: [(grad_value, variable1),(grad_value, variable2)]. Easy to use for newton_cg
def grads_vars(loss, params, opt):
  grads = opt.get_gradients(loss, params)
  grads_and_vars = list(zip(grads, params))
  return grads_and_vars

def execute_steps(
    func, initial_state, optimizer_class, optimizer_config, num_iter=500
):
    # Kill previous model
    tf.keras.backend.clear_session()
    tf.reset_default_graph()  
    
    # Set variables
    w1 = tf.Variable(initial_state[0], name= 'w1')
    w2 = tf.Variable(initial_state[1], name= 'w2')
    vars = [w1, w2]

    # we manually control tau
    # optimizer = optimizer_class(**optimizer_config) # For Adam, SGD
    if optimizer_class==es.EHNewtonOptimizer:
        optimizer = optimizer_class(tau= 1e-1, **optimizer_config) # For newton_cg
    else: 
        optimizer = optimizer_class(**optimizer_config)
    # loss
    loss = func(vars)

    # Get gradients and update the variables
    if optimizer_class==es.EHNewtonOptimizer:
        grads_and_vars = grads_vars(loss, vars, optimizer)  # For newton_cg
    else:
        grads_and_vars = optimizer.compute_gradients(loss, vars)  # For Adam, SGD
    for grad, var in grads_and_vars:
      grads_and_vars = [(tf.clip_by_norm(grad, 1.), var) for grad, var in grads_and_vars]

    # Set train
    train = optimizer.apply_gradients(grads_and_vars)
    
    # Run
    sess = tf.Session()
    sess.run(tf.global_variables_initializer())

    steps = []
    steps = np.zeros((2, num_iter + 1))
    steps[:, 0] = np.array(initial_state)
    sess = tf.Session()
    sess.run(tf.global_variables_initializer())
    for i in range(1, num_iter + 1):
      sess.run(train)
      steps[:, i] = sess.run(vars)  
    # print(steps)
    return steps

"""# objective_rosenbrock"""

def objective_rosenbrok(params):
    lr = params['lr']
    optimizer_class = params['optimizer_class']
    minimum = (1.0, 1.0)
    initial_state = (-2.0, 2.0)
    optimizer_config = dict(learning_rate=lr)
    print(optimizer_config)
    num_iter = 200
    steps = execute_steps(
        rosenbrock, initial_state, optimizer_class, optimizer_config, num_iter
    )
    return (steps[0][-1] - minimum[0]) ** 2 + (steps[1][-1] - minimum[1]) ** 2

"""# objective_rastrigin"""

def objective_rastrigin(params):
    lr = params['lr']
    optimizer_class = params['optimizer_class']
    initial_state = (-2.0, 3.5)
    minimum = (0, 0)
    optimizer_config = dict(learning_rate=lr)
    print(optimizer_config)
    num_iter = 200
    steps = execute_steps(
        rastrigin, initial_state, optimizer_class, optimizer_config, num_iter
    )
    return (steps[0][-1] - minimum[0]) ** 2 + (steps[1][-1] - minimum[1]) ** 2

"""# plot_rosenbrok"""

def np_rosenbrock(tensor):
    # https://en.wikipedia.org/wiki/Test_functions_for_optimization
    x, y = tensor
    return (1 - x) ** 2 + 100 * (y - x ** 2) ** 2

def plot_rosenbrok(grad_iter, optimizer_name, lr,color='r', option='x'):
    x = np.linspace(-2.5, 2.5, 250)
    y = np.linspace(-1.15, 3.5, 250)
    minimum = (1.0, 1.0)

    X, Y = np.meshgrid(x, y)
    Z = np_rosenbrock([X, Y])

    iter_x, iter_y = grad_iter[0, :], grad_iter[1, :]

    #fig = plt.figure(figsize=(8, 8))
    ax = plt.gca()
    #ax = fig.add_subplot(1, 1, 1)
    ax.contourf(X, Y, Z, 90, cmap='rainbow')
    ax.plot(iter_x, iter_y, color=color,marker=option)

    #ax.set_title(
    #    'Rosenbrock func: {} with {} '
    #    'iterations, lr={:.6}'.format(optimizer_name, len(iter_x), lr)
    #)
    #plt.plot(*minimum, 'gD')
    #plt.plot(iter_x[-1], iter_y[-1], 'rD')
    plt.savefig('rosenbrock.png'.format(optimizer_name))

"""# plot_rastrigin"""

# for plot
def np_rastrigin(tensor):
  x, y = tensor
  A = 10
  f = (
      A * 2
      + (x ** 2 - A * np.cos(x * math.pi * 2))
      + (y ** 2 - A * np.cos(y * math.pi * 2))
  )
  return f

def plot_rastrigin(grad_iter, optimizer_name, lr, color='r', option='x'):
    x = np.linspace(-4.5, 4.5, 250)
    y = np.linspace(-4.5, 4.5, 250)
    minimum = (0, 0)

    X, Y = np.meshgrid(x, y)
    Z = np_rastrigin([X, Y])

    iter_x, iter_y = grad_iter[0, :], grad_iter[1, :]

    #fig = plt.figure(figsize=(8, 8))

    ax = fig.add_subplot(1, 1, 1)
    ax.contourf(X, Y, Z, 20, cmap='rainbow')
    ax.plot(iter_x, iter_y, color=color,marker=option)
    #ax.set_title(
    #    'Rastrigin func: {} with '
    #    '{} iterations, lr={:.6}'.format(optimizer_name, len(iter_x), lr)
    #)
    #plt.plot(*minimum, 'gD')
    #plt.plot(iter_x[-1], iter_y[-1], 'rD')
    plt.savefig('rastrign.png'.format(optimizer_name))

"""# execute_experiments"""
def plot_square(grad_iter, optimizer_name, lr,color='r', option='x'):
    x = np.linspace(-2, 2, 250)
    y = np.linspace(-1, 3, 250)
    minimum = (0.0, 0.0)

    X, Y = np.meshgrid(x, y)
    Z = square([X, Y])

    iter_x, iter_y = grad_iter[0, :], grad_iter[1, :]

    #fig = plt.figure(figsize=(8, 8))

    ax = fig.add_subplot(1, 1, 1)
    ax.contourf(X, Y, Z, 90, cmap='rainbow')
    ax.plot(iter_x, iter_y, color=color,marker=option)

    #ax.set_title(
    #    'Square function x^2+4y^2: {} with {} '
    #    'iterations, lr={:.6}'.format(optimizer_name, len(iter_x), lr)
    #)
    #plt.legend(optimizer_name)
    #plt.plot(*minimum, 'gD')
    #plt.plot(iter_x[-1], iter_y[-1], 'rD')
    plt.savefig('square.png'.format(optimizer_name))

def execute_experiments(
    optimizers, objective, func, plot_func, initial_state, seed=1
):
    seed = seed
    for item in optimizers:
        optimizer_class, lr_low, lr_hi = item
        space = {
            'optimizer_class': hp.choice('optimizer_class', [optimizer_class]),
            'lr': hp.loguniform('lr', lr_low, lr_hi),
        }
        best = fmin(
            fn=objective,
            space=space,
            algo=tpe.suggest,
            max_evals=800,
            rstate=np.random.RandomState(seed),
        )
        print(best['lr'], optimizer_class)

        steps = execute_steps(
            func,
            initial_state,
            optimizer_class,
            {'learning_rate': best['lr']},
            num_iter=500,
        )
        plot_func(steps, optimizer_class.__name__, best['lr'])

"""# set optimizers"""

#optimizers = [tf.train.GradientDescentOptimizer, -8, 0.5)]
#optimizers = [(tf.train.AdamOptimizer, -8, 0.5)]
optimizers = [(es.EHNewtonOptimizer, -0, -1)]

"""# excute rastrigin"""

#execute_experiments(
#        optimizers,
#        objective_rastrigin,
#        rastrigin,
#        plot_rastrigin,
 #       (-2.0, 3.5)
  #      )

"""# excute_rosenbrock"""

#execute_experiments(
#        optimizers,
#        objective_rosenbrok,
#        rosenbrock,
#        plot_rosenbrok,
#        (-2.0, 2.0),
#    )

optimizer_class = es.EHNewtonOptimizer
lr = 0.02 # Best
# lr = 0.0199977
# lr = 0.0199867
# lr = 0.0001
steps1 = execute_steps(
            rosenbrock,
            (-2.0, 2.0),
            optimizer_class,
            {'learning_rate': lr},
            num_iter=501,
        )
steps2 = execute_steps(
            rosenbrock,
            (-2.0, 1.5),
            tf.train.GradientDescentOptimizer,
            {'learning_rate': lr},
            num_iter=501,
        )
steps3 = execute_steps(
            rosenbrock,
            (-2.0, 2.5),
            tf.train.AdamOptimizer,
            {'learning_rate': lr},
            num_iter=501,
        )
#plot_rosenbrok(steps, optimizer_class.__name__, lr)
fig = plt.figure()
plot_rosenbrok(steps2, optimizer_class.__name__, lr,'g','+')
plot_rosenbrok(steps3, optimizer_class.__name__, lr,'b','x')
plot_rosenbrok(steps1, optimizer_class.__name__, lr,'orange','*')
minimum=(1,1)
plt.plot(*minimum, 'b*')
ax = plt.gca() #ax = fig.add_subplot(1, 1, 1)
ax.annotate('min',xy=minimum)
proxy = [plt.Rectangle((0,0),1,1,fc = pc.get_facecolor()[0]) 
    for pc in ax.collections]

#fig.legend(proxy, ['Gradient Descent','adam', 'NCG'])#, loc='lower right')
#plt.clabel(ax, inline=1, fontsize=10)
fig.legend( ['(S)GD','adam', 'NCG'],bbox_to_anchor=(1.02, 1.0) ,loc='upper right')
#plt.tight_layout()

tikzplotlib.save("steps.tex")


"""# check oscillate"""

print(steps1[:][:30])

fig = plt.figure(figsize=(8, 6))
x_ = [ i for i in range(400, 430)]
plt.plot(x_, steps1[0][400:430],'x')
plt.plot(x_, steps2[0][400:430],'o')
plt.xlabel('iter', fontsize=12)
plt.ylabel('value at x axis', fontsize=12)
plt.show()
plt.savefig('opt.pdf')
